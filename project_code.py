# -*- coding: utf-8 -*-
"""Project code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l8VATVv0sOo-65hqu2UFdioPUui9BhNO
"""

#Deep learning neural network core with 80 convolutional layers.
#This code has been adapted from "Gao M, Skolnick J. A novel sequence alignment algorithm based on deep learning of the protein folding code"
#This can be used to train pairwise protein sequence with small percent identity.

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# Function to load protein sequences from a text file. Adapted for fasta file format
def load_sequences(file_path):
    sequences = []
    with open(file_path, 'r') as file:
        lines = file.readlines()
        i = 0
        while i < len(lines):
            if lines[i].startswith('>'):
                header = lines[i].strip()[1:]  # Remove '>'
                sequence = ''
                i += 1
                while i < len(lines) and not lines[i].startswith('>'):
                    sequence += lines[i].strip()
                    i += 1
                sequences.append((header, sequence))
            else:
                i += 1
    return sequences

# Function to convert amino acid sequence to one-hot encoding
def sequence_to_one_hot(sequence):
    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
    one_hot_sequence = np.zeros((len(sequence), len(amino_acids)))

    for i, aa in enumerate(sequence):
        if aa in amino_acids:
            one_hot_sequence[i, amino_acids.index(aa)] = 1

    return one_hot_sequence

# Load protein sequences
file_path = 'Pair_6.txt'
sequences = load_sequences(file_path)

# Convert sequences to one-hot encoding
one_hot_sequences = [sequence_to_one_hot(seq) for seq in sequences]

# Pad sequences to a fixed length (e.g., 665 residues)
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(one_hot_sequences, maxlen=665, padding='post', truncating='post')

# Reshape to match the input shape of the model
input_data = np.stack(padded_sequences, axis=0)

# Add channel dimension
input_data = np.expand_dims(input_data, axis=-1)

# Now, input_data has the shape (1, 2, 665, 20, 1)

# Display the shape of the input data
print("Input Data Shape:", input_data.shape)



# Define labels for each sequence (example with 3 classes)
# Assuming you have 2 sequences and each label is a sequence of integers
labels_matrix = [
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    [2, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
]

# Convert list of lists to NumPy array
labels_array = np.array(labels_matrix)

# Trim the labels_array to match the sequence length (665 residues)
labels_array = labels_array[:, :665]

# Convert labels to integers
labels_array = labels_array.astype(np.int32)



# Create the CNN model. Hidden layers defined with 2 convolutional layers.
def residual_block(x):
    x_conv1 = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x_conv2 = layers.Conv2D(64, (3, 3), padding='same', activation='linear')(x_conv1)
    x_relu1 = layers.Activation('relu')(x_conv2)
    x_relu2 = layers.Activation('relu')(x_relu1)
    x_add = layers.add([x_relu2, x])
    return x_add

def create_model(input_shape):
    inputs = tf.keras.Input(shape=input_shape)

    # Change the order of dimensions
    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(inputs)

    for _ in range(80):
        x = residual_block(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(22, activation='softmax')(x)  # Change 22 to your actual number of classes

    model = models.Model(inputs, outputs)
    return model

# Assuming input shape (20, 665, 1) where 20 is the number of channels
input_shape_cnn = (20, 665, 1)
model_cnn = create_model(input_shape_cnn)


# Define a callback to save the weights during training
checkpoint_callback = callbacks.ModelCheckpoint('C:/Users/tammy/Documents/School-GMU/BINF 730-BIO SEQUENCE ANALYSIS/Project',
                                                save_weights_only=True,
                                                save_best_only=True,
                                                verbose=1)

# Compile the model with sparse categorical cross-entropy
model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Fit the model with integer labels
model_cnn.fit(input_data, labels_array, epochs=10, batch_size=32, callbacks=[checkpoint_callback])

#CALCULATING E VALUE

import numpy as np
import math

def smith_waterman(distance_matrix, database_size):
    # Parameters
    gap_penalty = -2

    # Get the dimensions of the distance matrix
    n, m = distance_matrix.shape

    # Initialize the score matrix and traceback matrix
    score_matrix = np.zeros((n + 1, m + 1))
    traceback_matrix = np.zeros((n + 1, m + 1), dtype=int)

    # Fill the score matrix
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            match_score = score_matrix[i - 1, j - 1] + distance_matrix[i - 1, j - 1]
            delete_score = score_matrix[i - 1, j] + gap_penalty
            insert_score = score_matrix[i, j - 1] + gap_penalty

            # Choose the maximum score
            score_matrix[i, j] = max(0, match_score, delete_score, insert_score)

            # Update the traceback matrix
            if score_matrix[i, j] == match_score:
                traceback_matrix[i, j] = 1  # Diagonal
            elif score_matrix[i, j] == delete_score:
                traceback_matrix[i, j] = 2  # Up
            elif score_matrix[i, j] == insert_score:
                traceback_matrix[i, j] = 3  # Left

    # Find the maximum score and its position
    max_score = np.max(score_matrix)
    max_position = np.unravel_index(np.argmax(score_matrix), score_matrix.shape)

    # Calculate E-value
    query_length = max(n, m)
    alignment_length = max_position[0] + max_position[1]
    e_value = calculate_e_value(alignment_score, query_length, database_size, alignment_length)

    # Traceback to find the alignment
    aligned_sequence1 = []
    aligned_sequence2 = []

    i, j = max_position
    while i > 0 and j > 0 and score_matrix[i, j] > 0:
        if traceback_matrix[i, j] == 1:  # Diagonal
            aligned_sequence1.append(i - 1)
            aligned_sequence2.append(j - 1)
            i -= 1
            j -= 1
        elif traceback_matrix[i, j] == 2:  # Up
            i -= 1
        elif traceback_matrix[i, j] == 3:  # Left
            j -= 1

    # Reverse the aligned sequences
    aligned_sequence1.reverse()
    aligned_sequence2.reverse()

    return aligned_sequence1, aligned_sequence2, max_score, e_value

def calculate_e_value(alignment_score, query_length, database_size, alignment_length):
    k = 0.1  # Scaling factor, you may need to adjust this based on your data. Scaling improves accuracy
    lamda = 0.267  # Karlin-Altschul parameter, you may need to adjust this based on your data

    e_value = k * database_size * query_length * math.exp(-lamda * alignment_score / query_length)

    return e_value

# Example usage:
# Assuming distance_matrix is your 2D distance matrix
# Replace this with your actual distance matrix


# Assuming database_size is the size of your database
# Replace this with the actual size of your database
database_size = 10000

aligned_seq1, aligned_seq2, alignment_score, e_value = smith_waterman(distance_matrix, database_size)

print("Aligned Sequence 1:", aligned_seq1)
print("Aligned Sequence 2:", aligned_seq2)
print("Alignment Score:", alignment_score)
print("E-value:", e_value)